{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aymeric BEILLON et Antoine BERTIN**\n",
    "\n",
    "# **Introduction**\n",
    "----------------\n",
    "\n",
    "### Le but de ce projet est de créer un modèle de classification supervisée, permettant, à partir d'un jeu de données d'une compagnie d'assurance de classer des données. \n",
    "Ces données prennent la forme de profils clients, ces profils comprennent de nombreuses informations comme par exemple l'âge, le genre ou le credit score de l'assuré. La donnée importante pour la création de notre modèle et la connaissance de si, oui ou non, l'assuré à fait une demande d'indemnisation, cela pourra permettre à cette compagnie d'assurance de prévoir à l'avance selon le profil de nouveau clients, si ils feront une nouvelle demande. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 1 | IMPORTATION ET LECTURE DES DONNÉES\n",
    "------\n",
    "Ce premier bout de code permet de récupérer et lire les données depuis notre fichier CSV afin d'avoir une première analyse superficielle, ce faisant, nous pouvons vérifier la forme que prennent ces dites données. Ainsi nous pouvons préciser l'objectif du problème, il faut analyser les données pour prédire la demande d'indemnisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation des données\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "\n",
    "\n",
    "first_rows, info, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 2 | EXAMEN DES DONNÉES\n",
    "-------\n",
    "Dans cette partie l'objectif est de déceler, en analysant via des fonctions python, les valeurs manquantes et ou aberrantes présentes dans notre jeu de données. Ici, pour vérifier la présence potentielle de valeurs 'NA', nous allons utiliser la fonction isna() de Pandas, cette fonction permet de retourner ceci sur un DataFrame. Associée à la fonction sum(), nous pouvons retourner le nombre total de valeurs nulles sur chaque colonne. Il est également important de pouvoir vérifier les valeurs aberrantes, pour ce faire nous utilisons la méthode max() de python, permettant de retourner la valeur maximum de chaque colonne. Cette étape de vérification des données est essentielle puisqu'elle permet de savoir les actions à réaliser durant la phase de préparation des données. Pour vérifier les valeurs aberrantes, nous pouvons utiliser un box plot, qui représente les données de manière plus claire pour voir la présence de ces valeurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importation des données\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "maxi = data.max()\n",
    "\n",
    "\n",
    "# Détection des valeurs manquantes\n",
    "missing_values = data.isna().sum()\n",
    "data = data.drop(columns=['id'])\n",
    "\n",
    "data.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(50, 10))\n",
    "plt.show()\n",
    "\n",
    "first_rows, info, description, missing_values\n",
    "\n",
    "\n",
    "## Affichage du nombre de données manquantes pour chaque colonne\n",
    "print(missing_values)\n",
    "print(maxi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous pouvons ainsi repérer deux colonnes (credit_score et annual_mileage) qui comporte respectivement 982 et 957 valeurs manquantes. \n",
    "Afin de pouvoir travailler avec ces colonnes, il existe plusieurs solutions. Celles-ci peuvent être: \n",
    "- Suppression de la colonne\n",
    "- Imputation des valeurs manquantes à une valeur médiane\n",
    "- Mise à la valeur la plus courante\n",
    "\n",
    "Dans notre cas, le plus intéressant sera l'imputation à une valeur médiane afin de conserver le plus de cohérence sur les données\n",
    "\n",
    "Nous observons également des valeurs dites \"aberrantes\", qui sont trop élevées sans faire de sens, pour celle-ci, nous appliquerons une méthode permettant de limiter les valeurs entre des bornes. Ce box plot permet de voir clairement les valeurs aberrantes, pusique nous pouvons voir une valeur d'enfant au delà de 100, ceci ne faisant pas de sens, il sera nécessaire de la modifier dans la préparation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 3 | PREPARATION DES DONNEES\n",
    "-----\n",
    " \n",
    "Comme mentionné précédemment, nos données contiennent des valeurs manquantes ('NA') et des valeurs aberrantes. Dans cette phase de préparation des données, nous allons imputer ces valeurs et lisser le jeu de données afin d'obtenir un dataset cohérent, sur lequel nous pourrons travailler.\n",
    "\n",
    "Pour ce faire en Python, nous utiliserons des méthodes provenant du package scikit-learn, qui contient de nombreuses fonctions utiles pour manipuler les jeux de données. Pour imputer les valeurs à la médiane, nous utiliserons la classe SimpleImputer, qui permet de choisir la méthode d'imputation à utiliser. Ici, nous opterons pour l'imputation à la valeur médiane.\n",
    "\n",
    "Les lignes de code suivantes montrent comment transformer les valeurs manquantes en utilisant les règles définies par notre imputeur. \n",
    "\n",
    "Ensuite, pour limiter les valeurs aberrantes, nous utiliserons une fonction lambda (une méthode anonyme). Cette fonction lambda applique le nombre minimum entre la valeur présente dans la colonne et un seuil fixé (10 pour speeding_violations et 5 pour children)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "\n",
    "#suppression de la valeur inutile\n",
    "data = data.drop(columns=['id'])\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "\n",
    "print(first_rows)\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "data['children'] = data['children'].apply(lambda x: min(x, 5))\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "data.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(50, 10))\n",
    "\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', LabelEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "print(missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir préparé les données nous pouvons de nouveau les analyser comme fait dans la partie précédente. \n",
    "Nous n'avons ainsi plus de valeurs nulles, ni de valeurs aberrantes, notre jeu de données est donc prêt pour les parties suivantes:  la recherche de corellations et l'entraînement de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 4 | RECHERCHE DE CORRÉLATION\n",
    "----------\n",
    "Un jeu de données est un ensemble complet de données utilisé pour entraîner un modèle. Pour évaluer la performance de ce modèle, on le teste souvent sur un jeu de données de validation, qui est un sous-ensemble distinct non utilisé lors de l'entraînement. La séparation entre le jeu de données d'entraînement et celui de validation se fait de manière totalement aléatoire, afin d'obtenir des résultats représentatifs.\n",
    "\n",
    "\n",
    "En entraînant le modèle sur le premier jeu, on ajuste ses paramètres pour obtenir les meilleures prédictions possibles. Ensuite, on utilise le jeu de validation pour tester le modèle et s'assurer qu'il génère des résultats fiables sur des données non vues. Tester, c'est explorer les capacités du modèle pour découvrir ses points forts et faibles, tandis que valider consiste à prouver que les prédictions du modèle sont correctes et généralisables.\n",
    "\n",
    "\n",
    "Une bonne analogie serait celle d'un élève en mathématiques qui s'entraîne avec des exercices préparatoires (les TD) avant de passer un examen (le DS). Après l'entraînement initial, il peut s'exercer davantage avec d'autres exercices similaires pour s'assurer qu'il maîtrise bien les concepts, de la même manière qu'on utilise des données de validation pour confirmer la robustesse d'un modèle entraîné.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 20))\n",
    "data['children'] = data['children'].apply(lambda x: min(x, 5))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "numeric_data = data[['age', 'credit_score', 'annual_mileage', 'speeding_violations', 'duis', 'past_accidents', 'outcome','vehicle_year']]\n",
    "\n",
    "cor = numeric_data.corr()\n",
    "scatter_matrix = pd.plotting.scatter_matrix(numeric_data, figsize=(15, 15))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "sns.heatmap(cor, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 5 | EXTRACTION DES JEUX D'APPRENTISSAGE ET DE TEST\n",
    "----------\n",
    "Pour tester et valider notre algorithme nous pouvons procéder de deux manières.\n",
    "\n",
    "la première en entraînant notre algorithme avec une partie des données et ensuite valider l'entraînement des données avec l’autre partie.\n",
    "\n",
    "On peut partir sur une base de 75% de données d'entraînement et ensuite 25% de données de validation. Cette répartition est assez arbitraire. En effet nous ne possédons pas un nombre données illimités et donc si on entraîne notre algorithme avec peu de données pour ne pas sur entraîner notre algorithme. Ce qui donnerait des résultats erronés. \n",
    "\n",
    "Pour faire ceci en Python, nous allons utiliser des méthodes provenant du package scikit-learn, voici leur fonctionnement: \n",
    "Tout d'abord, nous séparons les données en variables explicatives et variable cible (ici X représente les variables explicatives et Y la cible). Dans X, nous conservons toute la table en supprimant seulement l'ID, ici inutile, et l'outcome. X représente donc les variables explicatives (features) utilisées pour prédire l'issue tandis que Y est la variable cible, qui est la colonne outcome du DataFrame data. C'est ce que nous voulons prédire.\n",
    "\n",
    "\n",
    "\n",
    "Nous utilisons par la suite *train_test_split(X,y, random_state=100)* Cette fonction de scikit-learn divise les données en ensembles d'entraînement et de test.\n",
    "X et y sont les variables explicatives et la variable cible respectivement.\n",
    "random_state=100 fixe la graine du générateur de nombres aléatoires pour que la division soit reproductible. Si on réexécute le code, nous obtenons les mêmes ensembles d'entraînement et de test. Cela nous sort donc :\n",
    "\n",
    "- X_train : Contient les variables explicatives pour l'ensemble d'entraînement.\n",
    "- X_test : Contient les variables explicatives pour l'ensemble de test.\n",
    "- y_train : Contient les étiquettes de l'ensemble d'entraînement.\n",
    "- y_test : Contient les étiquettes de l'ensemble de test.\n",
    "\n",
    "*preprocessor.fit_transform(X_train)*:\n",
    "\n",
    "*preprocessor* est un objet de type *ColumnTransformer* qui applique des transformations spécifiées sur les colonnes de *X_train*.\n",
    "fit_transform ajuste les transformateurs spécifiés dans preprocessor aux données d'entraînement *X_train* et applique les transformations.\n",
    "Cela signifie que les données d'entraînement sont transformées en fonction des spécifications données (par exemple, imputation, encodage).\n",
    "\n",
    "*preprocessor.transform(X_test)* :\n",
    "\n",
    "*preprocessor* utilise les mêmes paramètres (calculés lors de *fit_transform* sur *X_train*) pour transformer *X_test*.\n",
    "Cela garantit que les transformations appliquées aux données de test sont cohérentes avec celles appliquées aux données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des packages\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Lecture du fichier CSV\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "# Détection des valeurs manquantes\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Séparation des données en variables explicatives et cible\n",
    "X = data.drop(['id', 'outcome'], axis=1)\n",
    "y = data['outcome']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "missing_values = data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 6 | ENTRAINEMENT D'UN MODELE \n",
    "------\n",
    "\n",
    "Maintenant que nous avons nos set d'entraînement et de tests, nous allons pouvoir les utiliser pour entraîner et tester notre modèle de régression logistique. Pour ce faire nous utilisons la méthode *LogisticRegression* du package scikit-learn, celle-ci va entraîner notre modèle. \n",
    "\n",
    "La ligne de code ***model = LogisticRegression(max_iter=2000)*** crée un modèle de régression logistique avec une configuration spécifique dans le contexte de la bibliothèque scikit-learn. La variable max_iter va spécifier le nombre maximum d'itérations que l'algorithme d'optimisation est autorisé à effectuer lors de l'entraînement du modèle. Par défaut, ce paramètre est souvent fixé à une valeur relativement basse (par exemple, 100), mais dans ce cas, il est défini à 2000 pour permettre à l'algorithme plus de temps pour converger vers une solution optimale.\n",
    "\n",
    "------\n",
    "\n",
    "## Réponses aux Questions : \n",
    "\n",
    "### Régression Logistique\n",
    "\n",
    "La régression logistique utilise le concept d'évidence pour transformer cette probabilité en une valeur plus facile à manipuler mathématiquement.\n",
    "\n",
    "**Fonction Logit: La fonction logit transforme la probabilité p en une valeur réelle non bornée. La transformation est donnée par: logit(p) = ln(p/1p)**\n",
    "\n",
    "### Hypothèse\n",
    "\n",
    "Cette hypothèse permet de transformer la probabilité en une forme linéairement optimisable, facilitant ainsi l'ajustement et la prédiction du modèle. On passe d’une proba à un nombre linéaire entre - et + l’infini. \n",
    "\n",
    "**Relation Linéaire : La régression logistique suppose que le logit de la probabilité peut être exprimé comme une combinaison linéaire des variables explicatives** \n",
    "\n",
    "### Fonction de Coût\n",
    "\n",
    "\n",
    "La fonction de coût utilisée pour la régression logistique est la log-vraisemblance négative. L'objectif est \n",
    "d'optimiser cette fonction pour ajuster les coefficients avec l’algorithme de Descente de Gradient\n",
    "Calcul du Gradient : Le gradient de la fonction de coût par rapport à chaque coefficient (biais et Poids)\n",
    "\n",
    "**Itération: Ces étapes sont répétées jusqu'à convergence (jusqu'à ce que les changements dans la fonction de coût deviennent très petits ou après un nombre fixé d'itérations. Avec les variables calculés en fonction des probas on en ressort des biais et des poids d’après des calculs qui sont faits pour minimiser la perte**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Phase d'Apprentissage\n",
    "\n",
    "Pendant la phase d'apprentissage de l'algorithme de régression logistique, les paramètres suivants sont calculés et ajustés :\n",
    "**Coefficients (Poids) des Variables Explicatives : Pour chaque variable explicative un coefficient est calculé. Ces coefficients représentent l'impact de chaque variable explicative sur la probabilité prédite de l'événement.**\n",
    "\n",
    "\n",
    "#### Terme d'Interception (Biais) :\n",
    "Un coefficient d'interception est également calculé. Ce terme est indépendant des variables explicatives et capture l'impact moyen sur la probabilité prédite en l'absence de toute variable explicative.\n",
    "Donc le calcul gradient initialise le poids, c'est-à dire toutes les variables et le bias (l'interception), un calcul sans toutes ses variables. Et dans la phase d’apprentissage Le biais est important pour la Flexibilité et améliorer la précision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "# Détection des valeurs manquantes\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Séparation des données en variables explicatives et cible\n",
    "X = data.drop(['id', 'outcome'], axis=1)\n",
    "y = data['outcome']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Visualisation des distributions des variables numériques\n",
    "\n",
    "\n",
    "first_rows, info, description, missing_values\n",
    "\n",
    "\n",
    "# Calcul des corrélations\n",
    "correlation_matrix = numeric_data.corr()\n",
    "model = LogisticRegression(max_iter=398)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 7 | EVALUATION D'UN MODÈLE\n",
    "-------\n",
    "Pour évaluer la performance de notre modèle, nous utilisons plusieurs métriques. L'accuracy en est des plus simples à comprendre, elle mesure la proportion des prédictions correctes par rapport à l'ensemble des prédictions. Elle est calculée en divisant le nombre de prédictions correctes par rapport au nombre total de prédictions. Une accuracy élevée indique que le modèle est généralement correct, mais elle peut être trompeuse si les classes sont déséquilibrées.\n",
    "\n",
    "La matrice de confusion offre une évaluation plus détaillée en montrant la répartition des prédictions correctes et incorrectes parmi les différentes classes. Elle se présente sous la forme d'une grille où chaque ligne représente les instances réelles d'une classe et chaque colonne représente les instances prédictes d'une classe. Cela permet de voir non seulement combien de prédictions ont été correctes, mais aussi de comprendre où les erreurs se produisent. Ainsi sur la première colonne nous voyons les Vrais positifs et les faux positifs, dans la deuxième les faux négatifs et les vrais négatifs.\n",
    "\n",
    "Enfin, le classification report fournit une vue d'ensemble des principales métriques de classification pour chaque classe, telles que la précision (precision), le rappel (recall) et le score F1. La précision est le ratio des vraies prédictions positives sur le total des prédictions positives, le rappel est le ratio des vraies prédictions positives sur le total des instances positives, et le score F1 est la moyenne harmonique de la précision et du rappel. Ces métriques offrent une compréhension plus nuancée de la performance du modèle, en particulier dans les cas où les classes sont déséquilibrées.\n",
    "\n",
    "En pratique, il s'agit de trouver un compromis entre l'accuracy et le rappel.\n",
    "\n",
    "Ainsi, nous obtenons une accuracy d'environ 81%, avec un score de précision de 71% pour le positif et 85% pour le négatif, de même le score f1 est de 66% pour le positif et 87% pour le négatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   10000 non-null  int64  \n",
      " 1   age                  10000 non-null  int64  \n",
      " 2   gender               10000 non-null  int64  \n",
      " 3   driving_experience   10000 non-null  object \n",
      " 4   education            10000 non-null  object \n",
      " 5   income               10000 non-null  object \n",
      " 6   credit_score         9018 non-null   float64\n",
      " 7   vehicle_ownership    10000 non-null  float64\n",
      " 8   vehicle_year         10000 non-null  object \n",
      " 9   married              10000 non-null  float64\n",
      " 10  children             10000 non-null  float64\n",
      " 11  postal_code          10000 non-null  int64  \n",
      " 12  annual_mileage       9043 non-null   float64\n",
      " 13  vehicle_type         10000 non-null  object \n",
      " 14  speeding_violations  10000 non-null  int64  \n",
      " 15  duis                 10000 non-null  int64  \n",
      " 16  past_accidents       10000 non-null  int64  \n",
      " 17  outcome              10000 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "(7500, 18) (2500, 18)\n",
      "Accuracy: 0.8108\n",
      "Matrice de confusion:\n",
      " [[1560  189]\n",
      " [ 284  467]]\n",
      "Rapport de classification:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.89      0.87      1749\n",
      "         1.0       0.71      0.62      0.66       751\n",
      "\n",
      "    accuracy                           0.81      2500\n",
      "   macro avg       0.78      0.76      0.77      2500\n",
      "weighted avg       0.81      0.81      0.81      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "# Détection des valeurs manquantes\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Séparation des données en variables explicatives et cible\n",
    "X = data.drop(['id', 'outcome'], axis=1)\n",
    "y = data['outcome']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Visualisation des distributions des variables numériques\n",
    "\n",
    "first_rows, info, description, missing_values\n",
    "\n",
    "# Création d'un DataFrame avec les données transformées pour les variables numériques uniquement\n",
    "numeric_data = data[['age', 'credit_score', 'annual_mileage', 'speeding_violations', 'duis', 'past_accidents', 'outcome','vehicle_year']]\n",
    "\n",
    "# Calcul des corrélations\n",
    "correlation_matrix = numeric_data.corr()\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Matrice de confusion:\\n\", conf_matrix)\n",
    "print(\"Rapport de classification:\\n\", classification_rep)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 8 | AMÉLIORATION DE L'ÉVALUATION\n",
    "---------\n",
    "\n",
    "La méthode de validation croisée est une technique essentielle en apprentissage automatique pour évaluer les performances d'un modèle de manière robuste. Cette méthode permet de découper aléatoirement le jeu de données d'entraînement en plusieurs sous-ensembles distincts, puis d'entraîner et d'évaluer le modèle en passes successives. À chaque passe, un bloc est réservé pour l'évaluation tandis que les blocs restants sont utilisés pour l'entraînement. Cela permet d'obtenir une estimation plus fiable de la performance du modèle sur l'ensemble des données.\n",
    "Dans scikit-learn, la fonction kfold permet de réaliser une validation croisée en k passes. Elle divise le jeu de données en k sous-ensembles (ou \"folds\") de taille approximativement égale. Une fois ce jeu divisé en folds nous pouvons appliquer la méthode cross_val_score qui entraîne le modèle sur chaque sous-ensemble. En analysant les résultats d'accuracy avec le modèle simple et avec le kfold, nous observons une amélioration de l'accuracy moyenne avec le kfold. Ceci est expliqué par la suppression du biais lié à l'apprentissage sur l'ensemble du modèle\n",
    "Ainsi nous observons une nette amélioration sur l'accuracy de notre modèle, avec une accuracy de base à 81%, nous passons à plus de 84% avec la méthode de validation croisée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   10000 non-null  int64  \n",
      " 1   age                  10000 non-null  int64  \n",
      " 2   gender               10000 non-null  int64  \n",
      " 3   driving_experience   10000 non-null  object \n",
      " 4   education            10000 non-null  object \n",
      " 5   income               10000 non-null  object \n",
      " 6   credit_score         9018 non-null   float64\n",
      " 7   vehicle_ownership    10000 non-null  float64\n",
      " 8   vehicle_year         10000 non-null  object \n",
      " 9   married              10000 non-null  float64\n",
      " 10  children             10000 non-null  float64\n",
      " 11  postal_code          10000 non-null  int64  \n",
      " 12  annual_mileage       9043 non-null   float64\n",
      " 13  vehicle_type         10000 non-null  object \n",
      " 14  speeding_violations  10000 non-null  int64  \n",
      " 15  duis                 10000 non-null  int64  \n",
      " 16  past_accidents       10000 non-null  int64  \n",
      " 17  outcome              10000 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "(7500, 18) (2500, 18)\n",
      "Accuracy: 0.8108\n",
      "Confusion Matrix:\n",
      " [[1560  189]\n",
      " [ 284  467]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.89      0.87      1749\n",
      "         1.0       0.71      0.62      0.66       751\n",
      "\n",
      "    accuracy                           0.81      2500\n",
      "   macro avg       0.78      0.76      0.77      2500\n",
      "weighted avg       0.81      0.81      0.81      2500\n",
      "\n",
      "Cross-validation scores: [0.85   0.8425 0.8465 0.845  0.834 ]\n",
      "Mean cross-validation score: 0.8435999999999998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "# Examen des premières lignes et des informations sur les colonnes\n",
    "first_rows = data.head()\n",
    "info = data.info()\n",
    "description = data.describe()\n",
    "\n",
    "# Détection des valeurs manquantes\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Séparation des données en variables explicatives et cible\n",
    "X = data.drop(['id', 'outcome'], axis=1)\n",
    "y = data['outcome']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "# Visualisation des distributions des variables numériques\n",
    "first_rows, info, description, missing_values\n",
    "\n",
    "# Création d'un DataFrame avec les données transformées pour les variables de la matrice de corrélation\n",
    "numeric_data = data[['age', 'credit_score', 'annual_mileage', 'speeding_violations', 'duis', 'past_accidents', 'outcome','vehicle_year']]\n",
    "\n",
    "# Calcul des corrélations\n",
    "correlation_matrix = numeric_data.corr()\n",
    "model = LogisticRegression(max_iter=7000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Validation croisée K-Fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=100)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 9 | COMPARAISON AVEC D'AUTRES ALGORITHMES\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores for Logistic Regression\n",
      "[0.81133333 0.822      0.80533333 0.82333333 0.804     ]\n",
      "Mean CV Score: 0.8131999999999999\n",
      "Logistic Regression Test Set Accuracy: 0.8108\n",
      "\n",
      "Cross Validation Scores for Perceptron\n",
      "[0.68266667 0.68266667 0.31733333 0.77866667 0.682     ]\n",
      "Mean CV Score: 0.6286666666666666\n",
      "Perceptron Test Set Accuracy: 0.6996\n",
      "\n",
      "Cross Validation Scores for K-Nearest Neighbors\n",
      "[0.78666667 0.79133333 0.77733333 0.78266667 0.77933333]\n",
      "Mean CV Score: 0.7834666666666666\n",
      "K-Nearest Neighbors Test Set Accuracy: 0.7808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv('car_insurance.csv')\n",
    "\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['credit_score'] = imputer.fit_transform(data[['credit_score']])\n",
    "data['annual_mileage'] = imputer.fit_transform(data[['annual_mileage']])\n",
    "\n",
    "# Limitation des valeurs aberrantes\n",
    "data['speeding_violations'] = data['speeding_violations'].apply(lambda x: min(x, 10))\n",
    "data['children'] = data['children'].apply(lambda x: min(x, 5))\n",
    "\n",
    "# Encodage des variables qualitatives\n",
    "categorical_features = ['driving_experience', 'education', 'income', 'vehicle_year', 'vehicle_type']\n",
    "encoder = LabelEncoder()\n",
    "data[categorical_features] = data[categorical_features].apply(encoder.fit_transform)\n",
    "\n",
    "# Transformation des données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), ['age', 'credit_score', 'annual_mileage']),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Séparation des données en variables explicatives et cible\n",
    "X = data.drop(['id', 'outcome'], axis=1)\n",
    "y = data['outcome']\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "# Transformation des ensembles d'entraînement et de test\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Initialisation des classifieurs\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=8000),\n",
    "    'Perceptron': Perceptron(max_iter=8000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Evaluation des classifieurs\n",
    "for name, clf in classifiers.items():\n",
    "    # Apprentissage du modèle\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Evaluation par validation croisée\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    # Affichage des scores\n",
    "    print(\"Cross Validation Scores for\", name)\n",
    "    print(scores)\n",
    "    print(\"Mean CV Score:\", np.mean(scores))\n",
    "\n",
    "    # Prédiction sur l'ensemble de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calcul et affichage de l'accuracy sur le test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} Test Set Accuracy: {accuracy:.4f}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
